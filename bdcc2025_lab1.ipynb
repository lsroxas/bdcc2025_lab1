{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586b0510-782f-4926-a89c-b5fb910af28e",
   "metadata": {},
   "source": [
    "# Analyzing OSS Projects under the Apache Software Foundation using GH Archive Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b834c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The Apache Software Foundation (ASF) is a non-profit organization that supports various open-source software projects. The ASF has a large number of projects, and understanding the activity and trends within these projects can provide valuable insights into the open-source ecosystem.\n",
    "This analysis aims to explore the activity of Apache projects using data from the GitHub Archive (GH Archive). The GH Archive provides a record of public GitHub events, which can be used to analyze the activity of repositories over time since 2011. The dataset encompasses bullions of events like commits, issues, pull requests, and more. this dataset precents an opportunity for large-sclae analysis of open-source projects. \n",
    "\n",
    "This report's objectives are:\n",
    "* To identify key activity metrics derivable from the GH Archite dataset. \n",
    "* Provide descriptive analytics on these metrics over time for a selection of projects under ASF.\n",
    "* Describe the patterns and trends associated with the different phases of an open-source project's lifecycle.\n",
    "* Develop data-informed heuristics suggesting optimal project adoption and migration points. \n",
    "\n",
    "\n",
    "## Data Collection and Loading\n",
    "\n",
    "### BigQuery Extract\n",
    "The data for this analysis was collected from the GH Archive. A copy updated daily copy of this dataset is available on Google BigQuery under the `gharchive` public dataset. The data set was extracted using the following SQL query:\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "type,\n",
    "payload, \n",
    "repo.name as repo_name,\n",
    "DATE(created_at) as event_date,\n",
    "EXTRACT(YEAR FROM created_at) as event_year,\n",
    "EXTRACT(MONTH FROM created_at) as event_month, \n",
    "FROM `githubarchive.year.202*`\n",
    "WHERE repo.name like 'apache/%'\n",
    "```\n",
    "Screenshot of the export job's steps are shown below:\n",
    "\n",
    "![Export Job](./images/bigquery_export.png)\n",
    "\n",
    "The results of the query was exported as parquet files on a Google Cloud Storage bucket and copied over the the `/home/ubuntu/lab1/data` folder of the EC2 instance used by the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8985bcbf-40d5-4b18-abeb-14c6d2ef1983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22G\t/home/ubuntu/lab1/data\n"
     ]
    }
   ],
   "source": [
    "!du -sh /home/ubuntu/lab1/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a39b6-8a4a-43ca-a974-ece89d2746ed",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8745faef-4413-449d-8861-5da28564002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession\n",
    "     .builder\n",
    "     .master('local[*]')\n",
    "     .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "     # .config(\"spark.sql.parquet.columnarReaderBatchSize\", \"1024\")\n",
    "     # .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00dbf65d-d3b7-4ad4-8a77-dc38014901f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import (BooleanType, LongType, TimestampNTZType, StringType, StructType, StructField)\n",
    "\n",
    "data = spark.read.parquet('./data/*.parquet', \n",
    "    schema=gharchive_schema,\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e92023c-f1cc-46cf-ab1f-09019006c142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- payload: string (nullable = true)\n",
      " |-- repo_name: string (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      " |-- event_year: long (nullable = true)\n",
      " |-- event_month: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41d37b-4ab6-4202-bfe4-da626b935093",
   "metadata": {},
   "source": [
    "### Filtering Analysis on the 10 most active ASF repositories\n",
    "\n",
    "Due to the size of the data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900d13a-d5ce-4b2a-8161-f6de4ac3cad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8464ce2-44a0-4faf-95f9-03013965a832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
